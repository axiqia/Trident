\documentclass{webofc}
\usepackage[varg]{txfonts}   % Web of Conferences font

\usepackage{float}
\usepackage[caption = false]{subfig}
%\usepackage[final]{graphicx}

\begin{document}

\title{Trident : An Automated System Tool for \\Collecting and Analyzing Performance Counters}

\author{\firstname{Servesh} \lastname{Muralidharan}\inst{1}\fnsep\thanks{\email{servesh.muralidharan@cern.ch}} \and
        \firstname{David} \lastname{Smith}\inst{1}\fnsep\thanks{\email{david.smith@cern.ch}}
}

\institute{CERN}

\abstract{%
Trident, a tool to use low level metrics derived from hardware counters to understand Core, Memory and I/O utilisation and bottlenecks. The collection of time series of these low level counters does not induce significant overhead to the execution of the application.

The Understanding Performance team is investigation on a new node characterization tool, `Trident', that can look at various low level metrics with respect to the Core, Memory and I/O. Trident uses a three pronged approach to analysing node's utilisation and understand the stress on different parts of the node based on the given job. Currently core metrics such as memory bandwidth, core utilization, active processor cycles, etc., are being collected. Interpretation of the data is often non intuitive. The tool converts the data into derived metrics that are then represented as a system wide top-down analysis that helps developers and site managers understand the application behavior without the need for in-depth expertise of architecture details.
}
%
\maketitle
%
\section{Motivation}
\label{sec:moti}

Application performance can be characterized by a myriad of tools and techniques. Some of them rely on hardware and software performance counters of a given system to analyze performance bottlenecks. These counters can come from performance monitoring units (PMUs) built into modern processors, memory controllers, etc., or from software counters present in the underlying operating system. The sheer variety of these counters makes it difficult to quickly narrow down to those associated with a particular performance bottleneck. To tackle this, an approach called as Top-Down Analysis~\cite{6844459} was developed based on the counters from Intel processor's PMU. In several cases, it was shown that this technique could be used to reliably identify inefficiencies in different areas of a processor that arise from an application.

The effectiveness of Top-Down analysis lies in the fast diagnosis of performance bottlenecks by investigating the derived metrics in a hierarchical manner. However, this approach only works once hotspots are identified in a finite code space that account for a large percentage of the execution time. After which Top-Down analysis can be applied to determine optimizations that alleviate, if present, the limitations from the different parts of the architecture. 

In most High-Energy Physics (HEP) codes though, hotspot profiling has been shown to be ineffective []. The reason being the sparse compute code that has been spread over several thousands of C++ classes due to the complex nature of the physics algorithms and implementation inefficiencies. Therefore its become a key challenge to identify where to focus optimization efforts and to estimate the gain that can be achieved from improving it. In such a scenario the need for a more qualitative profiling tool becomes a necessity in contrast to the classical fine grained profiling approach.

In this paper we propose Trident, a coarse grained profiling tool tailored to the HEP community. It can quickly identify performance bottlenecks during the runtime of an application by monitoring hardware and software counters. These counters are carefully chosen to simultaneously monitor the key performance metrics of core, memory or IO subsystem of a given node. This three pronged approach allows Trident to form a holistic utilization pattern of a given node in a quick and efficient manner. Tuning the interval of the monitoring allows Trident to be used as a node efficiency analysis tool or a more complex application behaviour characterization tool.

\section{Introduction}
\label{sec:intro}

The concept of performance counter monitoring is neither new nor novel, however in real world scenarios plenty of difficulties arise in effectively following this approach. First to reliably collect hardware counters is a challenge. This is because all of the approaches need support from both hardware i.e. presence of the counters, and software i.e. OS level drivers and configuration. Furthermore, security implications also limit the different ways that can be used to obtain this data as an unprivileged user. Even if overcoming all this was possible for a specific system, scaling these to work in a generic way across a large number of systems is challenging due to the vast variety of configurations. From all this it became apparent that the online counter data collection system also had to be light weight, secure and intelligent enough to detect the specific counters supported by the architecture. All of this had to be done in a non-obscure way that doesn't sacrifice configurability to support large variety of systems.

Once the ability to effectively collect the counter data was solved, the next challenge was to apply the formulas for several different derived metrics. In addition this offline analysis also had to perform some form data parsing and validation. Also some of the derived metrics depend on the configuration of the node i.e. no of hyper-threaded cores, memory channels, etc., So this information had to be passed on for offline analysis from the monitoring subsystem. 

In the rest of this paper we will describe the Trident design architecture in section~\ref{sec:design}, the extended Top-Down analysis in section[] and some of the results from a test workload in section~\ref{sec:results}.


\section{Design}
\label{sec:design}

Trident is essentially composed of two distinct functional units. One, a lightweight counter aggregation system that runs continuously and two, a complex offline parser to calculate the different derived metrics from the counter data for an extended Top-Down analysis. The counter aggregation system has to be extremely lightweight such that it has negligible overhead on the application being tested. The offline analysis however, is more complex and can take lot longer to perform the necessary calculations for the various derived metrics. 

An overview of the Trident system design is shown in figure\ref{fig:design}.

\begin{figure}
  \centering
  \includegraphics[width=0.7\linewidth]{Design.pdf}
\caption{Trident system design}
\label{fig:design}
\end{figure}

\subsection{Online Measurement}

In order to have little to no influence over the execution of the application, it is essential to collect data from the hardware and software counters with little to no overhead. At the same time the approach has to remain flexible and support systems with little to no software configuration changes. Furthermore for production servers this would need to work without the need for elevated privileges for recording data from the hardware counters. Due to this we were limited to only few choices. 

In order to test the functional idea behind Trident and for the design prototype, the easiest and simplest way to record hardware counters appeared to be piggy backing on the \textbf{perf} subsystem[] of the Linux kernel. After setting up a few simple kernel configuration parameters, it was possible to record counter data from userspace. While \textbf{perf} supports high level counter names, we found this to drastically vary between kernel versions and processors. In addition some of the high level counters that we need do not appear to be directly supported. So therefore we developed a simple architecture identification mechanism using libpfm [] and configured a dynamic set even counters that we validated and tested on a given architecture.

\subsection{Offline Analysis}

%\begin{figure}
%	\centering
%	\subfloat[fig 1]{\includegraphics[width=0.48\linewidth]{{cmd2.pmpe16.Trident.1.io}.png}}\hfill
%	\subfloat[fig 2]{\includegraphics[width=0.48\linewidth]{{cmd2.pmpe16.Trident.1.me}.png}}\\
%	\subfloat[fig 3]{\includegraphics[width=0.48\linewidth]{{cmd2.pmpe16.Trident.1.cb}.png}}\hfill
%	\subfloat[fig 4]{\includegraphics[width=0.48\linewidth]{{cmd2.pmpe16.Trident.1.co}.png}} 
%\caption{plots of....}
%\label{fig:fig}
%\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{ExtendedTopDownDiag.pdf}
\caption{plots of....}
\label{fig:ext_top_down}
\end{figure}

\section{Results}

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{{cmd2.pmpe16.Trident.1.co}.png}
\caption{plots of....}
\label{fig:at_cmd2_co}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{{cmd2.pmpe16.Trident.1.cb}.png}
\caption{plots of....}
\label{fig:at_cmd2_cb}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{{cmd2.pmpe16.Trident.1.me}.png}
\caption{plots of....}
\label{fig:at_cmd2_me}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{{cmd2.pmpe16.Trident.1.io}.png}
\caption{plots of....}
\label{fig:at_cmd2_io}
\end{figure}

\section{Conclusion}

%For tables use syntax in table~\ref{tab-1}.
%\begin{table}
%\centering
%\caption{Please write your table caption here}
%\label{tab-1}       % Give a unique label
%% For LaTeX tables you can use
%\begin{tabular}{lll}
%\hline
%first & second & third  \\\hline
%number & number & number \\
%number & number & number \\\hline
%\end{tabular}
%% Or use
%\vspace*{5cm}  % with the correct table height
%\end{table}
%
\bibliography{chep18}

\end{document}
